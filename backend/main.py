from fastapi import FastAPI, Depends, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os
import traceback
from typing import Optional, List
from enum import Enum

# Use try/except to handle both relative and absolute imports
try:
    # Try relative imports first (when run as a module)
    from .database import supabase
    from .google_drive import ingest_documents_from_drive
    from .pinecone_store import get_pinecone_store
    from .utils import handle_api_error, validate_required_env_vars, log_operation, create_success_response, validate_chunk_ids, APIError
    from .auth import verify_clerk_jwt
    from .agents import MultiAgentContractAnalysis
except ImportError:
    # Fall back to absolute imports (when run directly)
    from database import supabase
    from google_drive import ingest_documents_from_drive
    from pinecone_store import get_pinecone_store
    from utils import handle_api_error, validate_required_env_vars, log_operation, create_success_response, validate_chunk_ids, APIError
    from auth import verify_clerk_jwt
    from agents import MultiAgentContractAnalysis


app = FastAPI()

# Configure CORS
origins = [
    "http://localhost:3000",
    "http://localhost:3001",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class AnalysisType(str, Enum):
    GENERAL = "general"
    RISK = "risk"
    COMPLIANCE = "compliance"

class ProcessRequest(BaseModel):
    prompt: str
    analysis_type: Optional[AnalysisType] = AnalysisType.GENERAL
    # columns are now dynamically generated by the AI

class IngestRequest(BaseModel):
    folder_id: str
    # user_id is now obtained from the JWT, so it's removed from the request body

@app.post("/api/process")
def process_prompt(request: ProcessRequest, user_id: str = Depends(verify_clerk_jwt)):
    """
    Process a user prompt to extract structured information from documents.
    
    Uses Pinecone for vector search and Anthropic Claude with LangGraph agents 
    for intelligent information extraction.
    """
    try:
        log_operation("Document Analysis Request", {
            "user_id": user_id,
            "prompt_length": len(request.prompt),
            "analysis_type": request.analysis_type
        })

        # Validate environment
        validate_required_env_vars("PINECONE_API_KEY", "ANTHROPIC_API_KEY")

        # 1. Create matrix record
        try:
            matrix_response = supabase.table("matrices").insert({
                "user_id": user_id,
                "prompt": request.prompt,
                "columns": []
            }).execute()
            matrix_id = matrix_response.data[0]["id"]
            log_operation("Matrix Created", {"matrix_id": matrix_id})
        except Exception as e:
            raise APIError(f"Failed to create analysis matrix: {str(e)}", "MATRIX_CREATION_ERROR")

        # 2. Search Pinecone for relevant chunks
        try:
            pinecone_store = get_pinecone_store()
            search_results = pinecone_store.search_similar_chunks(
                query=request.prompt,
                top_k=10,
                score_threshold=0.5
            )
            
            relevant_chunk_ids = [result['id'] for result in search_results]
            
            log_operation("Pinecone Search", {
                "chunks_found": len(search_results),
                "chunk_ids": relevant_chunk_ids[:3]  # Log first 3 IDs
            })
            
        except Exception as e:
            raise APIError(f"Vector search failed: {str(e)}", "VECTOR_SEARCH_ERROR")

        # Validate search results
        validate_chunk_ids(relevant_chunk_ids)

        if not relevant_chunk_ids:
            raise APIError("No relevant document chunks found", "NO_CHUNKS_FOUND")

        # 3. Use Multi-Agent System for Analysis
        try:
            log_operation("Starting Multi-Agent Analysis", {
                "query": request.prompt,
                "chunks_found": len(relevant_chunk_ids),
                "matrix_id": matrix_id
            })

            # Initialize multi-agent system
            multi_agent = MultiAgentContractAnalysis()

            # Run the complete multi-agent workflow
            result = multi_agent.analyze_documents(
                user_query=request.prompt,
                matrix_id=matrix_id
            )

            if result.get("error"):
                raise APIError(f"Multi-agent analysis failed: {result['error']}", "MULTI_AGENT_ERROR")

            generated_columns = result["columns"]
            extracted_data = result["data"]

            log_operation("Multi-Agent Analysis Complete", {
                "columns_generated": len(generated_columns),
                "documents_processed": len(extracted_data),
                "matrix_id": matrix_id
            })

        except APIError:
            raise  # Re-raise API errors as-is
        except Exception as e:
            raise APIError(f"Multi-agent analysis failed: {str(e)}", "MULTI_AGENT_ERROR")

        # Return successful response
        return create_success_response(
            "Document analysis completed successfully",
            {
                "columns": generated_columns,
                "data": extracted_data,
                "matrix_id": matrix_id,
                "chunks_analyzed": len(relevant_chunk_ids)
            }
        )

    except APIError as e:
        raise handle_api_error(e, "process_prompt")
    except Exception as e:
        raise handle_api_error(e, "process_prompt")

@app.post("/api/analyze-specialized")
def analyze_specialized(request: ProcessRequest, user_id: str = Depends(verify_clerk_jwt)):
    """
    This endpoint performs specialized analysis using different agent types (risk, compliance, etc.)
    based on the analysis_type parameter.
    """
    try:
        # Validate environment
        validate_required_env_vars("PINECONE_API_KEY", "ANTHROPIC_API_KEY")

        # Create matrix record for specialized analysis
        try:
            matrix_response = supabase.table("matrices").insert({
                "user_id": user_id,
                "prompt": request.prompt,
                "columns": []
            }).execute()
            matrix_id = matrix_response.data[0]["id"]
        except Exception as e:
            raise APIError(f"Failed to create analysis matrix: {str(e)}", "MATRIX_CREATION_ERROR")

        # Use multi-agent system for all analysis types
        multi_agent = MultiAgentContractAnalysis()

        # Add analysis type context to the prompt
        enhanced_prompt = f"[{request.analysis_type.value.upper()} ANALYSIS] {request.prompt}"

        result = multi_agent.analyze_documents(
            user_query=enhanced_prompt,
            matrix_id=matrix_id
        )

        if result.get("error"):
            raise APIError(f"Specialized analysis failed: {result['error']}", "SPECIALIZED_ANALYSIS_ERROR")

        return create_success_response(
            f"{request.analysis_type.value.title()} analysis completed successfully",
            {
                "analysis_type": request.analysis_type.value,
                "columns": result["columns"],
                "data": result["data"],
                "matrix_id": matrix_id
            }
        )

    except APIError as e:
        raise handle_api_error(e, "analyze_specialized")
    except Exception as e:
        raise handle_api_error(e, "analyze_specialized")

@app.post("/api/ingest-documents")
def ingest_documents(request: IngestRequest, user_id: str = Depends(verify_clerk_jwt)):
    """
    This endpoint triggers the ingestion of documents from a specified Google Drive folder.
    """
    try:
        result = ingest_documents_from_drive(request.folder_id, user_id)
        return result
    except Exception as e:
        traceback.print_exc() # Print traceback for debugging
        raise HTTPException(status_code=500, detail=f"Error during document ingestion: {e}")

@app.get("/api/documents")
def get_documents():
    try:
        # Query document_chunks table from Supabase
        response = supabase.table("document_chunks").select("*").execute()
        return {"document_chunks": response.data}
    except Exception as e:
        traceback.print_exc() # Print traceback for debugging
        return {"error": str(e)}

@app.get("/api/pinecone-stats")
def get_pinecone_stats(user_id: str = Depends(verify_clerk_jwt)):
    """
    Get statistics about the Pinecone vector index
    """
    try:
        pinecone_store = get_pinecone_store()
        stats = pinecone_store.get_index_stats()
        return {
            "vector_store": "pinecone",
            "index_name": os.getenv("PINECONE_INDEX_NAME", "contract-documents"),
            "stats": stats
        }
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error getting Pinecone stats: {e}")

if __name__ == "__main__":
    import uvicorn
    from main import app
    
    print("üöÄ Starting Contract Analysis Backend...")
    print("üìç Backend URL: http://localhost:8000")
    print("üìñ API Docs: http://localhost:8000/docs")
    print("üîß Make sure your .env file is configured!")
    print("-" * 50)
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )