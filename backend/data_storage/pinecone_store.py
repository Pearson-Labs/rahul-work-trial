import os
import uuid
from typing import List, Dict, Any, Optional
from pinecone import Pinecone, ServerlessSpec
from dotenv import load_dotenv
try:
    from .database import supabase
except ImportError:
    from database import supabase
import json

load_dotenv()

class PineconeVectorStore:
    def __init__(self):
        self.pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
        self.index_name = os.getenv("PINECONE_INDEX_NAME", "contract-documents")
        # Using 1024 dimensions to match multilingual-e5-large embeddings  
        self.dimension = 1024  
        self.metric = "cosine"
        self.embedding_model = "multilingual-e5-large"  # Try Pinecone first, fallback to Gemini
        
        # Initialize index if it doesn't exist
        self._ensure_index_exists()
        self.index = self.pc.Index(self.index_name)
    
    def _ensure_index_exists(self):
        """Create index if it doesn't exist"""
        try:
            existing_indexes = [index.name for index in self.pc.list_indexes()]
            
            if self.index_name not in existing_indexes:
                print(f"Creating Pinecone index: {self.index_name}")
                self.pc.create_index(
                    name=self.index_name,
                    dimension=self.dimension,
                    metric=self.metric,
                    spec=ServerlessSpec(
                        cloud=os.getenv("PINECONE_CLOUD", "aws"),
                        region=os.getenv("PINECONE_REGION", "us-east-1")
                    )
                )
                print(f"Index {self.index_name} created successfully")
            else:
                print(f"Index {self.index_name} already exists")
        except Exception as e:
            print(f"Error ensuring index exists: {e}")
            raise
    
    def _generate_embedding(self, text: str) -> List[float]:
        """Generate embedding using available methods"""
        try:
            # Try Pinecone's inference API first
            response = self.pc.inference.embed(
                model=self.embedding_model,
                inputs=[text],
                parameters={"input_type": "passage"}
            )
            return response[0]['values']
        except Exception as pinecone_error:
            print(f"Pinecone embedding failed: {pinecone_error}")
            print("Falling back to Gemini embeddings...")
            
            # Fallback to Gemini embeddings
            try:
                import google.generativeai as genai
                api_key = os.getenv("GEMINI_API_KEY")
                if not api_key:
                    raise ValueError("GEMINI_API_KEY not found for fallback")
                
                genai.configure(api_key=api_key)
                response = genai.embed_content(
                    model="models/embedding-001",
                    content=text,
                    task_type="RETRIEVAL_DOCUMENT"
                )
                
                # Pad or truncate to match expected dimensions (1024)
                embedding = response['embedding']
                if len(embedding) != self.dimension:
                    if len(embedding) > self.dimension:
                        # Truncate to 1024 dimensions
                        embedding = embedding[:self.dimension]
                    else:
                        # Pad with zeros to reach 1024 dimensions
                        embedding.extend([0.0] * (self.dimension - len(embedding)))
                
                return embedding
                
            except Exception as gemini_error:
                print(f"Gemini fallback failed: {gemini_error}")
                raise Exception(f"Both Pinecone and Gemini embedding failed. Pinecone: {pinecone_error}, Gemini: {gemini_error}")

    def add_document_chunk(self, chunk_text: str, metadata: Dict[str, Any], chunk_id: Optional[str] = None) -> str:
        """
        Add a document chunk to Pinecone with embedding generated by Pinecone
        
        Args:
            chunk_text: The text content of the chunk
            metadata: Metadata about the document and chunk
            chunk_id: Optional chunk ID, will generate if not provided
            
        Returns:
            The chunk ID that was used
        """
        try:
            if chunk_id is None:
                chunk_id = str(uuid.uuid4())
            
            # Generate embedding using Pinecone's inference API
            embedding = self._generate_embedding(chunk_text)
            
            # Prepare metadata for Pinecone (ensure all values are strings/numbers)
            pinecone_metadata = {
                "chunk_id": chunk_id,
                "chunk_text": chunk_text[:1000],  # Truncate for metadata storage
                "original_file_name": str(metadata.get("original_file_name", "")),
                "chunk_index": int(metadata.get("chunk_index", 0)),
                "total_chunks": int(metadata.get("total_chunks", 1)),
                "file_type": str(metadata.get("file_type", "")),
                "user_id": str(metadata.get("user_id", "")),
                "created_at": str(metadata.get("created_at", "")),
            }
            
            # Upsert to Pinecone
            self.index.upsert(vectors=[
                {
                    "id": chunk_id,
                    "values": embedding,
                    "metadata": pinecone_metadata
                }
            ])
            
            # Store in Supabase for full text and additional metadata (no embedding backup needed)
            supabase_data = {
                "id": chunk_id,
                "chunk_text": chunk_text,
                "metadata": metadata
            }
            
            supabase.table("document_chunks").upsert(supabase_data).execute()
            
            print(f"Successfully added chunk {chunk_id} to Pinecone and Supabase")
            return chunk_id
            
        except Exception as e:
            print(f"Error adding document chunk: {e}")
            raise
    
    def search_similar_chunks(self, query: str, top_k: int = 5, score_threshold: float = 0.7) -> List[Dict[str, Any]]:
        """
        Search for similar document chunks using Pinecone's vector similarity
        
        Args:
            query: The search query
            top_k: Number of results to return
            score_threshold: Minimum similarity score
            
        Returns:
            List of similar chunks with metadata and scores
        """
        try:
            # Generate embedding for the query using Pinecone
            query_embedding = self._generate_embedding(query)
            
            # Search in Pinecone
            search_response = self.index.query(
                vector=query_embedding,
                top_k=top_k,
                include_metadata=True,
                include_values=False
            )
            
            results = []
            for match in search_response.matches:
                if match.score >= score_threshold:
                    # Get full chunk text from Supabase using the chunk_id
                    chunk_response = supabase.table("document_chunks").select("*").eq("id", match.id).execute()
                    
                    if chunk_response.data and len(chunk_response.data) > 0:
                        chunk_data = chunk_response.data[0]
                        result = {
                            "id": match.id,
                            "score": match.score,
                            "chunk_text": chunk_data["chunk_text"],
                            "metadata": chunk_data["metadata"],
                            "pinecone_metadata": match.metadata
                        }
                        results.append(result)
            
            print(f"Found {len(results)} similar chunks for query: {query[:50]}...")
            return results
            
        except Exception as e:
            print(f"Error searching similar chunks: {e}")
            return []
    
    def delete_chunk(self, chunk_id: str) -> bool:
        """Delete a chunk from both Pinecone and Supabase"""
        try:
            # Delete from Pinecone
            self.index.delete(ids=[chunk_id])
            
            # Delete from Supabase
            supabase.table("document_chunks").delete().eq("id", chunk_id).execute()
            
            print(f"Successfully deleted chunk {chunk_id}")
            return True
            
        except Exception as e:
            print(f"Error deleting chunk {chunk_id}: {e}")
            return False
    
    def get_index_stats(self) -> Dict[str, Any]:
        """Get statistics about the Pinecone index"""
        try:
            stats = self.index.describe_index_stats()
            return {
                "total_vector_count": stats.total_vector_count,
                "dimension": stats.dimension,
                "index_fullness": stats.index_fullness,
                "namespaces": stats.namespaces
            }
        except Exception as e:
            print(f"Error getting index stats: {e}")
            return {}
    
    def search_by_metadata(self, filter_dict: Dict[str, Any], top_k: int = 100) -> List[Dict[str, Any]]:
        """
        Search chunks by metadata filters
        
        Args:
            filter_dict: Dictionary of metadata filters
            top_k: Maximum number of results
            
        Returns:
            List of matching chunks
        """
        try:
            # Convert filter to Pinecone format
            pinecone_filter = {}
            for key, value in filter_dict.items():
                pinecone_filter[key] = {"$eq": str(value)}
            
            # Query with filter only (no vector search)
            response = self.index.query(
                vector=[0.0] * self.dimension,  # Dummy vector
                top_k=top_k,
                filter=pinecone_filter,
                include_metadata=True,
                include_values=False
            )
            
            results = []
            for match in response.matches:
                # Get full data from Supabase
                chunk_response = supabase.table("document_chunks").select("*").eq("id", match.id).execute()
                
                if chunk_response.data and len(chunk_response.data) > 0:
                    chunk_data = chunk_response.data[0]
                    results.append({
                        "id": match.id,
                        "chunk_text": chunk_data["chunk_text"],
                        "metadata": chunk_data["metadata"]
                    })
            
            return results
            
        except Exception as e:
            print(f"Error searching by metadata: {e}")
            return []
    
    def bulk_upsert_chunks(self, chunks: List[Dict[str, Any]]) -> List[str]:
        """
        Bulk upsert multiple chunks using Pinecone's embedding generation
        
        Args:
            chunks: List of dictionaries with 'chunk_text', 'metadata', and optional 'chunk_id'
            
        Returns:
            List of chunk IDs that were upserted
        """
        try:
            vectors_to_upsert = []
            supabase_data = []
            chunk_ids = []
            
            # Collect all texts for batch embedding generation
            texts = [chunk["chunk_text"] for chunk in chunks]
            
            # Generate embeddings in batch using available methods
            print(f"Generating embeddings for {len(texts)} chunks...")
            try:
                # Try Pinecone batch embedding first
                embeddings_response = self.pc.inference.embed(
                    model=self.embedding_model,
                    inputs=texts,
                    parameters={"input_type": "passage"}
                )
                embeddings = [item['values'] for item in embeddings_response]
                print("Using Pinecone embeddings")
            except Exception as pinecone_error:
                print(f"Pinecone batch embedding failed: {pinecone_error}")
                print("Falling back to individual Gemini embeddings...")
                
                # Fallback to individual embedding generation
                embeddings = []
                for text in texts:
                    embedding = self._generate_embedding(text)
                    embeddings.append(embedding)
            
            for i, chunk in enumerate(chunks):
                chunk_id = chunk.get("chunk_id", str(uuid.uuid4()))
                chunk_ids.append(chunk_id)
                
                # Get the embedding for this chunk
                embedding = embeddings[i]
                
                # Prepare Pinecone vector
                pinecone_metadata = {
                    "chunk_id": chunk_id,
                    "chunk_text": chunk["chunk_text"][:1000],
                    "original_file_name": str(chunk["metadata"].get("original_file_name", "")),
                    "chunk_index": int(chunk["metadata"].get("chunk_index", 0)),
                    "user_id": str(chunk["metadata"].get("user_id", "")),
                }
                
                vectors_to_upsert.append({
                    "id": chunk_id,
                    "values": embedding,
                    "metadata": pinecone_metadata
                })
                
                # Prepare Supabase data (no embedding storage needed)
                supabase_data.append({
                    "id": chunk_id,
                    "chunk_text": chunk["chunk_text"],
                    "metadata": chunk["metadata"]
                })
            
            # Batch upsert to Pinecone (max 100 vectors per request)
            batch_size = 100
            for i in range(0, len(vectors_to_upsert), batch_size):
                batch = vectors_to_upsert[i:i + batch_size]
                self.index.upsert(vectors=batch)
            
            # Batch upsert to Supabase
            if supabase_data:
                supabase.table("document_chunks").upsert(supabase_data).execute()
            
            print(f"Successfully bulk upserted {len(chunk_ids)} chunks to Pinecone")
            return chunk_ids
            
        except Exception as e:
            print(f"Error in bulk upsert: {e}")
            return []

# Global instance
_pinecone_store = None

def get_pinecone_store() -> PineconeVectorStore:
    """Get or create global Pinecone store instance"""
    global _pinecone_store
    if _pinecone_store is None:
        _pinecone_store = PineconeVectorStore()
    return _pinecone_store